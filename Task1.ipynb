{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We initially experimented with logistic regression, RoBERTa-base, and RoBERTa-large.\n",
        "\n",
        "The following code implements the final model, an ensemble of DeBERTa-v3-large and ELECTRA-large-discriminator, which achieved the highest F1 score and outperformed all previous models.\n"
      ],
      "metadata": {
        "id": "puIJLt2BLmk1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LsU2TuCYX-Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "from scipy.stats import mode\n",
        "import transformers\n",
        "import sys\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PrintMetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_loss = None\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and 'loss' in logs:\n",
        "            self.train_loss = logs['loss']\n",
        "            print(f\"Step {state.global_step}: Train Loss: {logs['loss']:.4f}\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics is not None:\n",
        "            train_loss = self.train_loss if self.train_loss is not None else 'N/A'\n",
        "            val_loss = metrics.get('eval_loss', 'N/A')\n",
        "            eval_f1 = metrics.get('eval_f1', 'N/A')\n",
        "            print(f\"\\nEpoch | Training Loss | Validation Loss | F1\")\n",
        "            print(f\"{int(state.epoch)}     | {train_loss:.4f}       | {val_loss:.4f}        | {eval_f1:.4f}\\n\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "class ClickbaitDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512, is_test=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        # Concatenate postText, targetTitle, and targetParagraphs\n",
        "        text = f\"{row['postText']} [SEP] {row['targetTitle']} [SEP] {' '.join(row['targetParagraphs'])}\"\n",
        "        # Clean and format text\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Tokenizing the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "        # Return labels if it's not the test set\n",
        "        if not self.is_test:\n",
        "            item['labels'] = torch.tensor(row['label'], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "def compute_class_weights(df):\n",
        "    # Calculate the frequency of each class\n",
        "    class_counts = df['tags'].value_counts().sort_index()\n",
        "    total_samples = len(df)\n",
        "\n",
        "    # Compute class weights: inverse frequency\n",
        "    weights = []\n",
        "    for count in class_counts.values:\n",
        "        weight = total_samples / (len(class_counts) * count)\n",
        "        weights.append(weight)\n",
        "    # Apply square root to make weights less aggressive\n",
        "    weights = np.sqrt(weights)\n",
        "    return torch.tensor(weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
        "        labels = inputs.pop('labels')\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Using class-weighted cross-entropy loss\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    #F1 score calculation\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "    return {'eval_f1': f1}"
      ],
      "metadata": {
        "id": "ZMBhIznzYcyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIR = '/kaggle/input/dataset'\n",
        "OUTPUT_DIR = '/kaggle/working'\n",
        "TRAIN_PATH = os.path.join(INPUT_DIR, 'train.jsonl')\n",
        "VAL_PATH = os.path.join(INPUT_DIR, 'val.jsonl')\n",
        "TEST_PATH = os.path.join(INPUT_DIR, 'test.jsonl')\n",
        "\n",
        "\n",
        "for path in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    print(f\"Found: {path}\")\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "try:\n",
        "    train_data = pd.read_json(TRAIN_PATH, lines=True)\n",
        "    val_data = pd.read_json(VAL_PATH, lines=True)\n",
        "    test_data = pd.read_json(TEST_PATH, lines=True)\n",
        "    print(\"Data loaded successfully\")\n",
        "    print(\"Training samples:\", len(train_data))\n",
        "    print(\"Validation samples:\", len(val_data))\n",
        "    print(\"Test samples:\", len(test_data))\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to load JSON files: {e}\")\n",
        "\n",
        "# Preprocess tags column to handle lists\n",
        "def preprocess_tags(tags):\n",
        "    if isinstance(tags, list):\n",
        "        return tags[0] if tags else np.nan\n",
        "    return tags\n",
        "\n",
        "# Apply preprocessing to 'tags' column in training and validation datasets\n",
        "train_data['tags'] = train_data['tags'].apply(preprocess_tags)\n",
        "val_data['tags'] = val_data['tags'].apply(preprocess_tags)\n",
        "\n",
        "print(\"Sample tags (train):\", train_data['tags'].head(10).tolist())\n",
        "print(\"Tags types (train):\", train_data['tags'].apply(type).value_counts())\n",
        "print(\"Unique tags (train):\", train_data['tags'].unique())\n",
        "\n",
        "\n",
        "if train_data['tags'].isna().any() or val_data['tags'].isna().any():\n",
        "    raise ValueError(\"Found NaN values in tags after preprocessing\")\n",
        "\n",
        "# Label encoding: Convert tags into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['label'] = label_encoder.fit_transform(train_data['tags'])\n",
        "val_data['label'] = label_encoder.transform(val_data['tags'])\n",
        "\n",
        "print(\"Encoded labels:\", label_encoder.classes_)\n",
        "\n",
        "# Compute class weights to handle class imbalance\n",
        "class_weights = compute_class_weights(train_data)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oD-hbvHqYend"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    {\"name\": \"microsoft/deberta-v3-large\", \"max_length\": 512},\n",
        "    {\"name\": \"google/electra-large-discriminator\", \"max_length\": 512}\n",
        "    # {\"name\": \"roberta-large\", \"max_length\": 512}  # for future work, we can try this one as well.\n",
        "]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(OUTPUT_DIR, \"results\"),\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=2e-5, #1e-5 for tuning\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_f1',\n",
        "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "    max_grad_norm=1.0,\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "metadata": {
        "id": "fidA94fmYgkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store the logits (predictions) for validation and test sets\n",
        "val_logits = []\n",
        "test_logits = []\n",
        "\n",
        "# Clean the output directory before saving new results (In kaggle, we have limited output storage)\n",
        "print(f\"Clearing previous output in {OUTPUT_DIR}...\")\n",
        "sys.stdout.flush()\n",
        "for item in os.listdir(OUTPUT_DIR):\n",
        "    item_path = os.path.join(OUTPUT_DIR, item)\n",
        "    if os.path.isfile(item_path):\n",
        "        os.remove(item_path)\n",
        "    elif os.path.isdir(item_path):\n",
        "        shutil.rmtree(item_path, ignore_errors=True)\n",
        "print(f\"Cleared {len(os.listdir(OUTPUT_DIR))} items from {OUTPUT_DIR}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "\n",
        "# Now train and evaluate each model\n",
        "for model_config in models:\n",
        "    model_name = model_config[\"name\"]\n",
        "    max_length = model_config[\"max_length\"]\n",
        "    print(f\"Training {model_name} at {pd.Timestamp.now()}...\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    try:\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=3, # We have 3 classes for spoiler types\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"Creating datasets for {model_name}...\")\n",
        "        sys.stdout.flush()\n",
        "        train_dataset = ClickbaitDataset(train_data, tokenizer, max_length)\n",
        "        val_dataset = ClickbaitDataset(val_data, tokenizer, max_length)\n",
        "        test_dataset = ClickbaitDataset(test_data, tokenizer, max_length, is_test=True)\n",
        "        print(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        training_args.output_dir = os.path.join(OUTPUT_DIR, f\"results_{model_name.split('/')[-1]}\")\n",
        "        training_args.logging_dir = os.path.join(OUTPUT_DIR, f\"logs_{model_name.split('/')[-1]}\")\n",
        "\n",
        "        print(f\"Trainer initialized for {model_name}, starting training...\")\n",
        "        sys.stdout.flush()\n",
        "        trainer = WeightedTrainer(\n",
        "            class_weights=class_weights,\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[PrintMetricsCallback()] # We can monitor the training process\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting trainer.train()...\")\n",
        "        sys.stdout.flush()\n",
        "        trainer.train()\n",
        "        print(\"Finished trainer.train()\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "        print(f\"Predicting for validation and test sets with {model_name}...\")\n",
        "        sys.stdout.flush()\n",
        "        val_pred = trainer.predict(val_dataset).predictions\n",
        "        test_pred = trainer.predict(test_dataset).predictions\n",
        "        print(f\"Logits shapes: Val={val_pred.shape}, Test={test_pred.shape}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Store the logits (predictions) for later use in ensembling\n",
        "        val_logits.append(val_pred)\n",
        "        test_logits.append(test_pred)\n",
        "\n",
        "\n",
        "        del model, trainer, tokenizer\n",
        "        torch.cuda.empty_cache() #Free up GPU memory\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error training {model_name}: {e}\")\n",
        "        continue\n"
      ],
      "metadata": {
        "id": "WY-JgN-8YjqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"Creating ensemble predictions...\")\n",
        "\n",
        "\n",
        "val_logits_stacked = np.hstack([logits for logits in val_logits])\n",
        "test_logits_stacked = np.hstack([logits for logits in test_logits])\n",
        "print(f\"val_logits_stacked shape: {val_logits_stacked.shape}\")\n",
        "print(f\"test_logits_stacked shape: {test_logits_stacked.shape}\")\n",
        "\n",
        "\n",
        "stacking_classifier = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
        "stacking_classifier.fit(val_logits_stacked, val_data['label'])\n",
        "\n",
        "final_test_preds = stacking_classifier.predict(test_logits_stacked)\n",
        "final_spoiler_types = label_encoder.inverse_transform(final_test_preds)\n",
        "\n",
        "\n",
        "print(\"Available columns in test_data:\", test_data.columns.tolist())\n",
        "\n",
        "\n",
        "try:\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test_data['uuid'],\n",
        "        'spoilerType': final_spoiler_types\n",
        "    })\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}. Using index as id if uuid is missing.\")\n",
        "    submission = pd.DataFrame({\n",
        "        'id': range(len(final_spoiler_types)),\n",
        "        'spoilerType': final_spoiler_types\n",
        "    })\n",
        "\n",
        "submission_path = os.path.join(OUTPUT_DIR, 'submission.csv')\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"Submission file created at {submission_path}\")\n",
        "\n",
        "print(\"Submission preview:\")\n",
        "print(pd.read_csv(submission_path).head())\n"
      ],
      "metadata": {
        "id": "Ul3Ga9DpYllF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}